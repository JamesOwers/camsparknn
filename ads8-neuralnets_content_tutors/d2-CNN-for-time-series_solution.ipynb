{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The return of Time Series\n",
        "\n",
        "### Can you use a CNN for Time Series Data?\n",
        "\n",
        "**Anwer:** Of course you can, if with some creativity. The secret is representing the time series as an \"image\".\n",
        "\n",
        "In this notebook we consider the problem of detecting fraud in credit card transaction but including a temporal component (when the transaction took place). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up the project\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Nothing new here\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Needed to build custom transformers in Sklearn\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "#our deep learning super power\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the dataset\n",
        "\n",
        "Load the dataset `data/creditcard.csv` into Pandas and inspect it. \n",
        "The data is available at [this link](https://s3-eu-west-1.amazonaws.com/humongousdata/creditcard.csv), save it somewhere appropriate like `./data/credicard.csv`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ccfd for \"credit-card fraud data\"\n",
        "ccfd = pd.read_csv('./data/creditcard.csv')\n",
        "ccfd.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, there are 3 types of features:\n",
        "\n",
        "- **Time**: these are ordinal values, not datetime objects. They follow a _partial order_ ($t_1\\le t_2\\le \\dots$) and we cannot use them as an index\n",
        "- **V[N]**: these are PCA features\n",
        "- **Amount**: transaction amount (in USD)\n",
        "- **Class**: 0 - Genuine/ 1 - Fraud\n",
        "\n",
        "As you already know, credit Card Fraud Detection is a *hard problem* because it is highly imbalanced. \n",
        "Use an appropriate plot to show the imbalance between the two classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "ccfd[\"Class\"].hist(alpha=0.4)\n",
        "plt.grid('off')\n",
        "plt.xticks([0, 1], [\"Non-Fraud\", \"Fraud\"], fontsize=12)\n",
        "plt.yticks([1e5, 2e5], [\"0.1\", \"0.2\"], fontsize=12)\n",
        "plt.ylabel(\"Number of occurences in millions\", fontsize=14)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploring imbalance\n",
        "\n",
        "Write a function `print_class_distribution` that takes a `y` vector and returns the ratio of 0s (Genuine) to 1s (Fraud). Apply it on the data. \n",
        "\n",
        "The function `np.bincount` is particularly useful here or you can also use `scipy.stats.itemfreq`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import itemfreq\n",
        "\n",
        "def print_class_distribution(ys):\n",
        "    # vectorise if needed\n",
        "    ys = ys.reshape(-1)\n",
        "    bins = np.bincount(ys)\n",
        "    print(\"Genuine: {} / Fraud: {} = {:.3f}%\".format(\n",
        "        bins[0], bins[1], 100 * bins[1] / np.sum(bins)))\n",
        "\n",
        "print_class_distribution(ccfd[\"Class\"].values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So **less than 0.5%** of the transactions are fraud.\n",
        "\n",
        "### Train-test split\n",
        "\n",
        "How are we going to split the data? Can we use the built-in sklearn function? Unfortunately not because you need to preserve the chronological order. \n",
        "\n",
        "The function below is a rudimentary implementation that takes a dataframe and returns a training and test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train_test_split_time_series(df, test_ratio=0.3, time=\"Time\", labels=\"Class\"):\n",
        "    # sort the values according to the time column\n",
        "    df.sort_values(time, inplace=True)\n",
        "    # number of rows\n",
        "    total_samples = df.shape[0]\n",
        "    # splitting index (e.g.: to take first 80% as training then test_ratio=0.2)\n",
        "    train_idx = int(total_samples * (1 - test_ratio))\n",
        "    # locating the relevant parts\n",
        "    XTrain = df.loc[:train_idx, df.columns != 'Class'].values\n",
        "    yTrain = df.loc[:train_idx, df.columns == 'Class'].values\n",
        "    XTest = df.loc[train_idx:, df.columns != 'Class'].values\n",
        "    yTest = df.loc[train_idx:, df.columns == 'Class'].values\n",
        "    \n",
        "    return XTrain, yTrain, XTest, yTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the function above on the dataframe and show the shapes of the objects created, check everything makes sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "XTrain, yTrain, XTest, yTest = train_test_split_time_series(ccfd)\n",
        "print(XTrain.shape)\n",
        "print(yTrain.shape)\n",
        "print(XTest.shape)\n",
        "print(yTest.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the proportion of fraud cases in the training and test set, make sure it kind of looks ok (you can use the function you defined earlier for this). \n",
        "\n",
        "**Question**: could you use stratification here? if so, how? and what is the underlying assumption with respect to the proportion of fraudulent transaction through time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_class_distribution(yTrain)\n",
        "print_class_distribution(yTest)\n",
        "print_class_distribution(ccfd[\"Class\"].values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can observe that there are proportionally more fraud samples into the training set. \n",
        "What is the implication of this? (discuss)\n",
        " \n",
        "### Preprocessing\n",
        "\n",
        "As per usual, it's a good idea to apply the standard scaler on columns.\n",
        "Note that usual PCA output may in fact already be scaled, so test this first and if unsatisfactory, apply a standard scaler (e.g.: via a Pipeline so that you can apply the same transformation on training and testing data). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline([('scaling', StandardScaler())])\n",
        "preprocessor = pipeline.fit(XTrain)\n",
        "XTrain_s = preprocessor.transform(XTrain)\n",
        "XTest_s = preprocessor.transform(XTest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the mean and variance of the transformed training set just to check it corresponds to your intuition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "checker = lambda x: (np.mean(x), np.var(x))\n",
        "for i in range(XTrain_s.shape[1]):\n",
        "    print(\"mean: {0:.2f}, var: {1:.2f}\".format(\n",
        "            np.abs(np.mean(XTrain_s[:, i])), \n",
        "            np.var(XTrain_s[:, i])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting a feel for the data\n",
        "\n",
        "Have a look at the first dimension (time), of the training data and show a histogram/distplot.  \n",
        "What can you observe and how do you interpret it? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.distplot(XTrain_s[:, 0])\n",
        "plt.xlabel(\"Time (normalised)\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What that histogram shows is effectively the _number of operations over a normalised timescale_ (left is earlier, right is later and, therefore, closer to the test set). \n",
        "So modes happen at times where there are a lot of transactions. \n",
        "\n",
        "It is a trimodal (*three modes*) distribution... How does that affect the design of our model? \n",
        "\n",
        "Note:\n",
        "- We do not know the time span of the period we are looking at\n",
        "- There could be different seasonality during each of the distinct time periods\n",
        "- We could be evaluating on a different distribution (i.e. the test set does not bear much statistical resemblance with the training set), this will lead to poor generalisation. \n",
        "\n",
        "Long story short, always be careful and ask yourself whether your test set allows you to make any strong statement about the generalisability of your model. \n",
        "This is a *hard question* in general with no simple rule of thumb. \n",
        "\n",
        "### Prepare data for CNN application\n",
        "\n",
        "The following function reshapes a matrix into a number of batches (smaller matrices with fewer rows and the same number of columns). \n",
        "Have a look at the code (and at the output) and check it makes sense. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def reshape_to_batches(matrix, batch_size):\n",
        "    # pad the matrix with zeros if the number of rows is not divisible by the batch_size\n",
        "    # np.ceil is the upper-rounding operator so np.ceil(4.3) == 5.0\n",
        "    batch_num = np.ceil(matrix.shape[0] / batch_size)\n",
        "    modulo = batch_num * batch_size - matrix.shape[0]\n",
        "    if modulo != 0: # not divisible by batch_size\n",
        "        # add some 0-rows to the matrix\n",
        "        padding = np.zeros((int(modulo), matrix.shape[1]))\n",
        "        matrix = np.vstack((matrix, padding))\n",
        "        \n",
        "    return np.array(np.split(matrix, batch_num))\n",
        "\n",
        "\n",
        "# Let's see how this works\n",
        "matrix = np.zeros((7, 5))\n",
        "print(matrix.shape)\n",
        "print(reshape_to_batches(matrix, 3).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do the same on the training data with batches of size 100. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(XTrain_s.shape) # original dimensions\n",
        "XTrain_s_batch = reshape_to_batches(XTrain_s, 100)\n",
        "\n",
        "print(XTrain_s_batch.shape) # now in batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More dimension adjustments\n",
        "\n",
        "Since we are going to use the `categorical_crossentropy` loss function (the standard loss for binary classification), we need to transform our class labels into a binary matrix of (1s and 0s) of shape `(samples, classes)`. \n",
        "Execute the cell below and make sure you understand the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_binary = to_categorical(yTrain)\n",
        "\n",
        "print(yTrain.shape)   # vector with 0 - 1 entries (-> 1 column)\n",
        "print(y_binary.shape) # 2 classes: genuine or fraud (-> 2 columns)\n",
        "\n",
        "print()\n",
        "print(\"First three rows before binarization...\")\n",
        "print(yTrain[:3])\n",
        "print(\"First three rows after binarization...\")\n",
        "print(y_binary[:3, :])\n",
        "\n",
        "yTrain_batch = reshape_to_batches(y_binary, 100)\n",
        "\n",
        "print()\n",
        "print(yTrain_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A CNN for the data\n",
        "\n",
        "We will try to apply a CNN on the data. \n",
        "Where we had used 2-D convolutions on images, we now use 1-D convolutions on a sequence but the principle is the same. \n",
        "Remember that a convolution in 2D is done with the following steps:\n",
        "\n",
        "- for a given patch size e.g.: `3x3`\n",
        "- for a given kernel `K` corresponding to that size (i.e. `(3, 3)`) \n",
        "- take patches of the given size on the image, e.g. let's call one `I` \n",
        "- compute the element-wise product of the entries of the patch `I` and the kernel `K` and sum:\n",
        "\n",
        "$$ \\sum_{r,s=1}^3 I_{rs} K_{rs} $$\n",
        "\n",
        "So the adaptation to something that operates in \"time\" instead of \"space\" is reasonably straightforward:\n",
        "\n",
        "- for a time segment size e.g.: 9 steps\n",
        "- for a given \"kernel\" corresponding to that size (i.e. `(9,)`)\n",
        "- take segments of the given size in the time series e.g. let's call one `I`\n",
        "- compute the element-wise product of the entries and the kernel and sum:\n",
        "\n",
        "$$ \\sum_{r=1}^9 I_r K_r $$\n",
        "\n",
        "So the principle is identical, we try to learn \"kernels\" which can pick up features / patterns over time windows of a given size.\n",
        "\n",
        "In practice, with Keras, you need to import `Conv1D` to do this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#import all dependencies\n",
        "from keras.layers import Input, Dense, Conv1D\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we need to define how the input will look like. \n",
        "For computational reasons, we want to process more transactions at a time, but not too many so that the transfer time outweights the computational gain. \n",
        "Hence, we are going to feed 100 transactions at a time, each with the 30 features. \n",
        "\n",
        "We can specify this using the `Input` constructor from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(100, 30)) # This returns a tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now comes the most important part!\n",
        "\n",
        "We use 1D Conv since we are only going to stride one way (along the time axis). \n",
        "We define 32 kernels (features) and a kernel size (sliding window) of 5. \n",
        "Note that the `strides` parameter indicates how much to shift the kernel, just as for an autoregressor.  \n",
        " \n",
        "The last important point is that we give the output of the previous layer as input to this layer `Conv1D(...)`.\n",
        "For this we use Keras' syntax to chain Layers:\n",
        "\n",
        "`A_Layer_Constructor(...options to define the new layer...)(the_previous_layer)` \n",
        "\n",
        "or, more specifically here\n",
        "\n",
        "`Conv1D(...)(inputs)`\n",
        "\n",
        "where inputs was the layer we just defined indicating the dimension of the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "\n",
        "conv1 = Conv1D(32, (5),           # 32 filters with a window of width 5\n",
        "               strides=1,         # think autoregression\n",
        "               padding='causal',  # forward in time\n",
        "               dilation_rate=1,   # ignore this and everything that follows are default parameters\n",
        "               activation='relu', \n",
        "               use_bias=True,\n",
        "               kernel_initializer='glorot_uniform', \n",
        "               bias_initializer='zeros',\n",
        "               kernel_regularizer=None, # no regularisation for the moment\n",
        "               bias_regularizer=None, \n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None, \n",
        "               bias_constraint=None)(inputs) # syntax to chain layers: Layer(...)(PreviousLayer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now add a fully-connected layer with 64 neurons after that and relu neurons (note that the choice of 64 is fairly arbitrary, we picked it to have something \"large but not too large\" but there's not much more than guesswork here as, unfortunately, with much of \"deep learning\"). \n",
        "Again, we chain that layer to the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "fc1 = Dense(64, activation='relu')(conv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, for the output, we need a softmax layer with 2 neurons (two classes: 0/1). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "predictions = Dense(2, activation='softmax')(fc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the task of credit card transaction fraud Detection is a binary classification problem, as an exercise, you can investigate the effect of changing the predictions layer from `softmax` to a 1 dimensional layer with a sigmoid activation (why?, what does it change?).\n",
        "\n",
        "Using Keras' functional API we can define the model and compile it with the loss etc (this part should feel familiar).\n",
        "Note that a model can have **more than one** inputs and outputs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# wrapping the model, mentioning the input and output layers\n",
        "model = Model(inputs=inputs, \n",
        "              outputs=predictions)\n",
        "# compiling the model, here we choose \"rmsprop\" to do the training but you could use Adam etc\n",
        "# the loss is the standard loss for classification and we want to show the accuracy.\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the model on the batched training data with 100 epochs, you should see the accuracy on the test set increase dramatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.fit(XTrain_s_batch, yTrain_batch, epochs=30) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation\n",
        "\n",
        "Let's see how are model is doing. \n",
        "\n",
        "Questions:\n",
        "1. what accuracy is reported here? (evaluated over what data?)\n",
        "1. is accuracy a good metric to use in our current use case?\n",
        "1. what would be a better evaluation metric?\n",
        "\n",
        "### Evaluation on the test set\n",
        "\n",
        "We need to apply the same pre-processing as on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "#first transform the test data into the appropriate shape\n",
        "print(XTest_s.shape)\n",
        "xTest_s_batch = reshape_to_batches(XTest_s, 100)\n",
        "print(xTest_s_batch.shape)\n",
        "y_binary = to_categorical(yTest)\n",
        "print(yTest.shape)\n",
        "print(y_binary.shape)\n",
        "yTest_batch = reshape_to_batches(y_binary, 100)\n",
        "print(yTest_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#make the prediction with the trained model\n",
        "y_pred = model.predict(xTest_s_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# store the raw predictions we will need them in a bit\n",
        "y_hat = np.copy(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the scikit learn implementation of precision, recall, f1, which means we need to reshape the tensors(N-dimensional vectors) into 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a function to reshape batches into the original shape\n",
        "def _3d_to_2d(arr):\n",
        "    return arr.reshape(arr.shape[0] * arr.shape[1], arr.shape[2])\n",
        "\n",
        "\n",
        "_3d_to_2d(y_pred).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sklearn metrics functions expect a single vector, containing either a probability score or a confidence interval. \n",
        "Further, since our labels are binary labels, we can only compare them if our results are also binary. \n",
        "Hence, we are going to make a simplifying assumption: all classifications where there is a higher than 50% chance for a given class are going to be assigned that class and vice versa. \n",
        "\n",
        "**Question**: is 50% a good threshold here? if not, why not?\n",
        "\n",
        "Let's stick with that threshold for now and we will revisit in a bit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "threshold = 0.5\n",
        "y_pred[np.where(y_pred >= threshold)] = 1\n",
        "y_pred[np.where(y_pred < threshold)]  = 0\n",
        "\n",
        "print(confusion_matrix(\n",
        "        _3d_to_2d(yTest_batch)[:, 1], \n",
        "        _3d_to_2d(y_pred)[:, 1]))\n",
        "\n",
        "print()\n",
        "\n",
        "print(classification_report(\n",
        "        _3d_to_2d(yTest_batch)[:, 1], \n",
        "        _3d_to_2d(y_pred)[:, 1],\n",
        "        target_names = [\"Genuine\", \"Fraud\"],\n",
        "        digits = 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is mostly pretty good but, as always with highly imbalanced data, you need to be careful.\n",
        "Remember that the confusion matrix reads as:\n",
        "\n",
        "|                | Genuine (pred) | Fraud (pred) |\n",
        "|----------------|----------------|--------------|\n",
        "| Genuine (true) | TP             | FP           |\n",
        "| Fraud (true)   | FN             | TN           |\n",
        "\n",
        "so there were quite a few fraudulent transactions that were considered \"fine\" by the algorithm which is really not what you want! (`FN` is large compared to `FN+TN`) the fraud recall `TN/(TN+FN)` is quite poor here even though everything else \"looks great\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROC or how performances change with threshold\n",
        "\n",
        "As hinted at above, the threshold was picked _arbitrarily_ at 50% and, as you may have guessed, the threshold is rather suboptimal in this case. \n",
        "The selection of threshold is a subtle operation which is best done using what is known as the \"ROC curve\". \n",
        "The ROC curve effectively computes the confusion matrix for a range of thresholds from 0 to 1 and displays the recall versus the Fall-Out or False Positive Rate (FPR). \n",
        "\n",
        "- True Positive Rate (TPR) also _Recall_ or _sensitivity_: `TP/(TP+FN)`\n",
        "- False Positive Rate (TNR): `FP/(FP+TN)`\n",
        "\n",
        "Try to think in a general sense for an arbitrary binary classifier about how this curve should look like reflecting about extreme cases:\n",
        "\n",
        "1. what happens if the threshold is set to something very close to `1` like `0.99`?\n",
        "1. what happens if the threshold is set to something very close to `0` like `0.01`?\n",
        "1. what is an ideal pair TPR/FPR?\n",
        "\n",
        "Once this curve is computed, the AUC or _area under the curve_ is a metric of performance of the algorithm.\n",
        "The ideal curve maximises the AUC, try to draw it and think about what it means. \n",
        "\n",
        "In the cell below, we compute the ROC and AUC for the classifier we just trained and for a range of thresholds. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "# long way, allows to plot the curve\n",
        "fpr, tpr, thresholds = roc_curve(_3d_to_2d(yTest_batch)[:, 1], \n",
        "                                 _3d_to_2d(y_hat)[:, 1])\n",
        "print(auc(fpr, tpr))\n",
        "\n",
        "# short way, gives the AUC directly\n",
        "print(roc_auc_score(_3d_to_2d(yTest_batch)[:, 1], \n",
        "                    _3d_to_2d(y_hat)[:, 1]))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = {0:.2f})'.format(auc(fpr, tpr)))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--') # random-guess baseline\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
        "plt.title('Receiver Operating Characteristic', fontsize=12)\n",
        "plt.legend(loc=\"lower right\", fontsize=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we save the curve to facilitate comparisons later on\n",
        "import pickle\n",
        "\n",
        "pickle.dump((fpr, tpr, thresholds, y_pred), \n",
        "            open(\"res_cnn.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the ROC curve, we can now **select a threshold**, depending on our requirements. \n",
        "It is now a **business decision**.\n",
        "\n",
        "For example, your client may say that they want the optimal performance for a maximum of 0.6% FPR. \n",
        "\n",
        "* Find the threshold where the false positive rate is larger or equal to 0.6%\n",
        "* Show the threshold as well as the corresponding FPR and TPR "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the value of FPR, where it is 0.6% or slightly above\n",
        "fpr_id = np.where(fpr >= 0.006)[0][0]\n",
        "\n",
        "print(\"FPR {:2.2f}%\".format(fpr[fpr_id]*100))\n",
        "print(\"TPR {:2.2f}%\".format(tpr[fpr_id]*100))\n",
        "print(\"threshold {:.2e}\".format(thresholds[fpr_id]*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's a pretty extreme threshold... \n",
        "\n",
        "Show the confusion matrix and classification report with this threshold and discuss\n",
        "\n",
        "- the precision\n",
        "- the recall\n",
        "- the f1-score\n",
        "\n",
        "in the context of a business where each undected fraud potentially costs `$$$$` whilst each false flag (flagging a genuine transaction as fraud) costs `$`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "cutt_off_tr = thresholds[fpr_id]\n",
        "y_pred[np.where(y_hat >= cutt_off_tr)] = 1\n",
        "y_pred[np.where(y_hat < cutt_off_tr)]  = 0\n",
        "\n",
        "print(confusion_matrix(\n",
        "            _3d_to_2d(yTest_batch)[:, 1],\n",
        "            _3d_to_2d(y_pred)[:, 1]))\n",
        "\n",
        "print(classification_report(\n",
        "        _3d_to_2d(yTest_batch)[:, 1], \n",
        "        _3d_to_2d(y_pred)[:, 1],\n",
        "        target_names = [\"Genuine\", \"Fraud\"],\n",
        "        digits=5))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}