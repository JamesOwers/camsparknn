{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolution Neural Networks\n",
        "\n",
        "## Part 1: Convolutions\n",
        "\n",
        "In this first part you will see what convolutions do and how they work. \n",
        "\n",
        "### Load the Python libraries\n",
        "\n",
        "Let us start by loading the necessary Python libraries and set a few parameters for the notebook. the `misc` element from `scipy` will allow us to do some elementary image manipulation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for elementary image manipulation\n",
        "from scipy import misc\n",
        "\n",
        "% matplotlib inline\n",
        "\n",
        "# specifies the default figure size for this notebook\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "# specifies the default color map\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import an image\n",
        "\n",
        "To explore how convolutions work, you will use the portrait of [Grace Hopper](https://en.wikipedia.org/wiki/Grace_Hopper).\n",
        "Use the `imread` function from `misc` to load the image. ([imread documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html) specify the option `flatten=True` which averages the three color channel into one for a black-white photo). \n",
        "\n",
        "The image corresponds to a matrix of dimension H x W (height by width) where each entry corresponds to a pixel value between 0 (black) and 255 (white).\n",
        "\n",
        "Use `imshow` from `plt` to display the image. \n",
        "Since it's a matrix, you can use standard numpy style indexing to only show a region.\n",
        "Show the first 200 lines and the first 600 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the grace_hopper.jpg image from the data folder\n",
        "\n",
        "# Show the image\n",
        "\n",
        "# Show another figure with only the first 200 rows / 600 cols\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can view the values of the pixels directly, for example select the first five rows and columns (**top left corner**) and show the corresponding matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the pixel values of a region in the top left corner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define and apply a convolution function\n",
        "\n",
        "Now lets define a convolution function. First you must define a function which traverses the image to apply the convolution at every point and returns the result in a filtered image. Calculating the size of the filtered image along each dimension can be a little tricky, the formula is: \n",
        "\n",
        "                         (Size of the filtered image) = (input image size) - (filter size) + 1\n",
        "\n",
        "Let us start by implementing the `convolve` function. It takes as input an image and a filter matrix, and returns\n",
        "the output of applying the filter at each position in the image through a function `multiply_sum`. \n",
        "\n",
        "The function `multiply_sum` corresponds to the following operation for two matrices $A$ and $B$ of identical dimensions:\n",
        "\n",
        "$$\n",
        "\\text{multiply_sum}(A, B) = \\sum_{i,j} A_{ij}B_{ij}\n",
        "$$\n",
        "\n",
        "i.e. you form a matrix $C$ with entries corresponding to the entry-wise products and you sum across $C$\n",
        "\n",
        "**Note**: the implentations here are computationally inefficient (and you will see that applying it on the image takes a second). In practice, when dealing with thousands of images, you really don't want sub-optimal operations which is why libraries like Tensorflow hide away *a lot* of optimisations to make such operations as quick as possible and leverage the hardware that is available to you (e.g.: GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# add your code to define the multiply_sum function (check that it works on a simple example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function below defines the convolution operation, go through the code and make sure it makes sense to you what is happening (there is a bit of fiddling required to apply the operations at the right place and store the results appropriately)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Convolution function\n",
        "def convolve(image, filter_matrix):\n",
        "    \n",
        "    # get the dimension of the filter\n",
        "    filter_height = filter_matrix.shape[0]\n",
        "    filter_width  = filter_matrix.shape[1]\n",
        "    \n",
        "    # allocate an empty array for the filtered image using the formula\n",
        "    # this is the array we'll use to store the result of the convolution\n",
        "    filtered_image = np.ndarray(shape=(image.shape[0] - filter_height + 1, \n",
        "                                       image.shape[1] - filter_width + 1))\n",
        "    \n",
        "    # go through rows\n",
        "    for row in range(filtered_image.shape[0]):\n",
        "        # go through columns\n",
        "        for col in range(filtered_image.shape[1]):\n",
        "            # select a local patch of the image\n",
        "            patch = image[row:(row + filter_height), \n",
        "                          col:(col + filter_width)]\n",
        "\n",
        "            # apply the multiply_sum operation\n",
        "            ms = multiply_sum(patch, filter_matrix)\n",
        "            \n",
        "            # store it at the right location\n",
        "            filtered_image[row, col] = ms\n",
        "            \n",
        "    return filtered_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An there you have it, a convolution operator! You can apply a filter onto an image and see the result. \n",
        "\n",
        "Define a filter matrix corresponding to\n",
        "\n",
        "$$\n",
        "\\left(\\begin{array}{ccc}\n",
        "    -1&-1&-1\\\\ \n",
        "    2&2&2\\\\ \n",
        "    -1&-1&-1\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "apply it to GH's portrait and display the result with `imshow`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Add your code here...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quiz: What did our filter do?\n",
        "\n",
        "1) By looking at the image, can you tell what kind of pattern the filter detected?\n",
        "\n",
        "2) How would you design a filter which detects vertical edges?\n",
        "\n",
        "3) What would the following filter do: ([Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator))\n",
        "\n",
        "$$\n",
        "\\left(\\begin{array}{ccc}\n",
        "    1&1&1\\\\ \n",
        "    0&0&0\\\\ \n",
        "    -1&-1&-1\n",
        "\\end{array}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "how about its transpose? how about if you swap the first and last row?\n",
        "\n",
        "Try variations until you get an intuition for what these operators do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the filter and see what it does then transpose it and check again\n",
        "\n",
        "# apply the transpose\n",
        "\n",
        "# flip first and last row\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convolutions with colour\n",
        "\n",
        "Very good! But what if we had a coloured image, how would we use that extra information to detect useful patterns? \n",
        "The idea is simple: on top of having a set weight for each pixel, we have a set weight for each colour channel within that pixel.\n",
        "Filters become stacks of kernels (usually 3 for the three channels: R, G, B). \n",
        "\n",
        "An example is the following kernel which detects region of the image that are mostly brown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "brown_filter = np.array(\n",
        "      [[[ 0.13871045,  0.17157242,  0.12934428], # Red channel\n",
        "        [ 0.16168842,  0.20229845,  0.14835016],\n",
        "        [ 0.135694  ,  0.16206263,  0.11727387]],\n",
        "\n",
        "       [[ 0.04231958,  0.05471011,  0.03167877], # Green channel\n",
        "        [ 0.0462575 ,  0.06581022,  0.03104937],\n",
        "        [ 0.04185439,  0.04734124,  0.02087744]],\n",
        "\n",
        "       [[-0.15704881, -0.16666673, -0.16600266], # Blue channel\n",
        "        [-0.17439997, -0.17757156, -0.18760149],\n",
        "        [-0.15435153, -0.17037505, -0.17269668]]])\n",
        "\n",
        "print(brown_filter.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **first** dimension corresponds the three channels (R, G, B) (try `brown_filter[1, :, :]` for the filter values corresponding to the green channel). \n",
        "\n",
        "Looking at the values, you can see that the filter responds to regions that are red (positive values, reasonably large), a little bit to the green values (positive values, quite small), and not at all to regions that are blue (negative values). \n",
        "\n",
        "To see it in practice, use `imread` from `misc` without `flatten=True` to load the coloured image of Grace Hopper.\n",
        "\n",
        "Show the image and its dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display the coloured image of Grace Hopper\n",
        "\n",
        "# Show the shape of the image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we would like to apply the Brown filter and see the result. \n",
        "One thing needs to be done, it's a bit annoying but it happens *all the time* in CNNs (and other non-trivial NNs): you need to adjust dimensions. \n",
        "Currently, it is the **first** dimension of the brown filter that corresponds to the colour channels while you saw that it is the **last** dimension of GH's image that correspond to the channels. \n",
        "\n",
        "Therefore you need to re-arrange dimensions from \n",
        "\n",
        "```\n",
        "(0, 1, 2) -----> (1, 2, 0)\n",
        "```\n",
        "\n",
        "this can be done via the `transpose` method: `array.transpose((1, 2, 0))`.\n",
        "\n",
        "Adjust the brown filter and apply it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adjust the dimensions of the brown filter and apply it to the image of GH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quizz\n",
        "\n",
        "Can you design a filter which will detect the edge from the background (blue) to Grace Hopper\u2019s left shoulder (black).\n",
        "\n",
        "**Note**: it's good practice to have the weights in your fi\u0080lter sum to 0 and don't forget to re-arrange the dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# devise a left_shouler_filter and apply it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Neural networks\n",
        "\n",
        "Now lets load an already trained network in our environment. This network (VGG-16) has been trained on the Imagenet dataset where the goal is to classify pictures into one out of one thousand categories. \n",
        "When it came out in 2014, it won the annual ImageNet Recognition Challenge correctly classifying 93% of the images in the test set. \n",
        "For comparison, humans can achieve around 95% accuracy. \n",
        "It's also very simple, it only uses 3x3 convolutions (like the ones you have used before)! \n",
        "\n",
        "It is however rather deep and it takes **2 to 3 weeks with 4 GPUs** to train it...\n",
        "\n",
        "To load the model, you must first define it's architecture. \n",
        "You're going to do this step by step as you learn the components of convolutional neural networks. \n",
        "\n",
        "**IMPORTANT NOTE**: it is extremely difficult to come up with an architecture \"that works\". So while people successfully adapt existing neural nets such as VGG16 to their needs (and, in fact, a lot of people use it without its last layer as an *image-embedding operator*), architecture design is the realm of research (and much head scratching).\n",
        "\n",
        "Recently, the Google Brain team has applied brute-force style search to try to find the best architectures for problems but also to learn what activation function to use, what step-size mechanisms etc (learn everything-approach). \n",
        "This required an obscene amount of resources and the results were far from intuitive (*search for \"Google Brain 2017 year review\" for a discussion of this and many other interesting results*). \n",
        "\n",
        "### Load the Python libraries\n",
        "\n",
        "Let's load the necessary libraries. We are again going to use the `Keras` library with the tensorflow backend. You will also use `cv2` for image manipulations. Keras and Tensorflow can be installed via `pip`, `cv2` also though you may have to write\n",
        "\n",
        "```bash\n",
        "pip install opencv-python\n",
        "```\n",
        "\n",
        "for it to work. \n",
        "\n",
        "**NOTE**: these libraries are fairly advanced in terms of computations and therefore it can be a bit fiddly to install them on your computer (especially Tensorflow and OpenCV). If it doesn't work, it's very likely someone has had your problem before so just copy paste the error message in Google and go from there though for this notebook we recommend you pair up with someone who has a working implementation (so that you don't waste too much time reading stackoverflow and github posts). You may also get a few warnings from Python but if it doesn't look too scary, you're probably fine. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import cv2 # for image manipulations\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.convolutional import ZeroPadding2D\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing a convolutional layer\n",
        "\n",
        "You are going to define the first convolutional layer of the network. But before, you will add some padding to the image so the convolutions get to apply on the outer edges.\n",
        "\n",
        "In what follows you don't have to modify the cells but just run them making sure you understand what is being done. Do not tune the parameters as we will load pre-trained weights on the architecture!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create the model, it's a Sequential model (stack of layers one after the other)\n",
        "vgg_model = Sequential()\n",
        "\n",
        "# On the very first layer, you must specify the input shape\n",
        "# ZeroPadding2D adds a frame of 0 (column left and right, row top and bottom)\n",
        "# the tuple (1, 1) indicates it's one pixel and symmetric.\n",
        "vgg_model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 3))) \n",
        "\n",
        "# Your first convolutional layer will have 64 3x3 filters, \n",
        "# and will use a relu activation function\n",
        "vgg_model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_1'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacking layers\n",
        "\n",
        "Now you're going to stack another convolutional layer. Remember, the output of a convolutional layer is a 3-D tensor, just like the input image. Although it does have a much higher depth!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Once again you must add padding\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(64, (3, 3), activation='relu', name='conv1_2'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding pooling layers\n",
        "\n",
        "Now lets add your first pooling layer. Pooling reduces the width and height of the input by aggregating adjacent cells together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Add a pooling layer with window size 2x2\n",
        "# The stride indicates the distance between each pooled window\n",
        "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding more convolutions for VGG\n",
        "\n",
        "Now you can stack many more of these! Remember not to change the parameters as we are about to load the weights of an already trained version of this network.\n",
        "\n",
        "Also, as you will quickly realise, Keras for practitioners usually means a lot of copy-pasting..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# second set of Padding - Conv - Padding - Conv - Pooling\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_1'))\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(128, (3, 3), activation='relu', name='conv2_2'))\n",
        "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# third set\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_1'))\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_2'))\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(256, (3, 3), activation='relu', name='conv3_3'))\n",
        "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# fourth set\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_1'))\n",
        "vgg_model.add(ZeroPadding2D((1,1)))\n",
        "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_2'))\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv4_3'))\n",
        "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# fifth set\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_1'))\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_2'))\n",
        "vgg_model.add(ZeroPadding2D((1, 1)))\n",
        "vgg_model.add(Conv2D(512, (3, 3), activation='relu', name='conv5_3'))\n",
        "vgg_model.add(MaxPooling2D((2, 2), strides=(2, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the depth of the layers get progressively larger, up to 512 for the latest layers. \n",
        "This means that, as we go along, each layer detects a greater number of features. \n",
        "\n",
        "On the other hand, each max-pooling layer halves the height and width of the layer outputs. \n",
        "Starting from images of dimensions 224x224, the final outputs are only of size 7x7.\n",
        "\n",
        "Now you're about to add some fully connected layers which can learn the more abstract features of the image. \n",
        "But first you must first change the layout of the input so it looks like a 1-D tensor (vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Flatten the output\n",
        "vgg_model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer with 4096 neurons\n",
        "vgg_model.add(Dense(4096, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Flatten` function removes the spatial dimensions of the layer output, it is now a simple 1-D vector of numbers. This means we can no longer apply 2D convolution layers as before, but we can apply fully connected layers like the ones of the perceptron from the previous module.\n",
        "\n",
        "`Dense` layers are fully connected layers. You used them in the previous module.\n",
        "\n",
        "### Preventing overfitting with Dropout\n",
        "\n",
        "`Dropout` is a method used at train time to prevent overfitting. As a layer, it randomly modifies its input\n",
        "so that the neural network learns to be robust to these changes. Although you won\u2019t actually use it\n",
        "now, you must define it to correctly load the pre-trained weights as it was part of the original network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Add a dropout layer\n",
        "vgg_model.add(Dropout(0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number 0.5 indicates the amount of change, 0.0 means no change, and 1.0 means completely diff\u0081erent.\n",
        "\n",
        "\n",
        "Add one more fully connected layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "vgg_model.add(Dense(4096, activation='relu'))\n",
        "vgg_model.add(Dropout(0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally a softmax layer to predict the categories. There are 1000 categories and hence 1000 neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "vgg_model.add(Dense(1000, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the weights\n",
        "\n",
        "And you're all set with the architecture! Let's load the weights of the network. For this use \n",
        "\n",
        "```python\n",
        "vgg_model.load_weights(path)\n",
        "```\n",
        "\n",
        "where `path` is the path to `vgg16_weights_tf_dim_ordering_tf_kernels.h5` which you can get [here](https://github.com/fchollet/deep-learning-models/releases): "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile the network no need to worry about this for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "sgd = SGD()\n",
        "vgg_model.compile(optimizer=sgd, loss='categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing the data\n",
        "\n",
        "Lets feed an image to your model. In the VGG network, we only do zero centering. The model takes as input a slightly transformed version of the input. \n",
        "\n",
        "1. use `cv2` to read the `cat.jpg` image in `data/` as well as the `puppy.jpg` and any other simple photo you found online\n",
        "2. resize the images to a 224 by 224 image using `cv2.resize`\n",
        "3. (optional) show the photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The VGG16 network assumes that the input it will receive is an array of 224 by 224 RGB images that have been *centered* in each of their channels. \n",
        "\n",
        "The function below applies the centering and makes sure the dimensions are right. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This transformation performs the 0-centering\n",
        "def transform_image(image):\n",
        "    image_t = np.copy(image).astype(np.float32) # Avoids modifying the original\n",
        "    image_t[:, :, 0] -= 103.939                 # Substracts mean Red\n",
        "    image_t[:, :, 1] -= 116.779                 # Substracts mean Green\n",
        "    image_t[:, :, 2] -= 123.68                  # Substracts mean Blue\n",
        "    image_t = np.expand_dims(image_t, axis=0)   # The network takes batches of images as input\n",
        "    return image_t\n",
        "\n",
        "img_t = transform_image(img)\n",
        "img2_t = transform_image(img2)\n",
        "\n",
        "print(img_t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first dimension is a \"dummy\" dimension, that is because the network expects an array of images as input. \n",
        "\n",
        "The three subsequent dimensions are the image dimensions with the three colour channels at the end. \n",
        "\n",
        "### Getting an output from the network\n",
        "\n",
        "Let's push the images through the network and see what happens!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Push the image through the network using vgg_model.predict call the result\n",
        "\n",
        "# the output is for a batch of images but you only gave one so extract the first element\n",
        "\n",
        "# now plot the output, xlabel=Categories, ylabel=Probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The network seems pretty confident! Lets look at its top 5 guesses for each of the two images. \n",
        "\n",
        "1. load the labels from `data/synset_words`\n",
        "2. sort the top 5 probabilities\n",
        "3. display the corresponding categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hurray! Our network knows what it's talking about. Let's have a closer look at what goes on inside.\n",
        "\n",
        "### Looking inside the network\n",
        "\n",
        "In a convolutional neural network, there's an easy way to visualise the filters learned at the very first layer. We can print each filter to show which colours it reponds to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a helper function to let you visualise what goes on inside the network\n",
        "\n",
        "def vis_square(weights, padsize=1, padval=0, activation=False):\n",
        "    # Avoids modifying the network weights\n",
        "    data = np.copy(weights)\n",
        "    \n",
        "    # Normalise the inputs\n",
        "    data -= data.min()\n",
        "    data /= data.max()\n",
        "    \n",
        "    # Lets tile the inputs\n",
        "    # How many inputs per row (e.g.: if 64 filters then 8x8)\n",
        "    \n",
        "    n = int(np.ceil(np.sqrt(data.shape[0]))) \n",
        "    \n",
        "    # add padding between inputs (you can safely ignore this)\n",
        "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
        "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
        "    \n",
        "    # place the filters on an n by n grid\n",
        "    data = data.reshape((n, n) + data.shape[1:])\n",
        "    \n",
        "    # merge the filters contents onto a single image\n",
        "    data = data.transpose((0, 2, 1, 3) + tuple(range(4, data.ndim)))\n",
        "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
        "        \n",
        "    # show the filter. In the activation case we don't have a colour channel (see further)\n",
        "    plt.figure()\n",
        "    if activation:\n",
        "        plt.imshow(data[:, :, 0])\n",
        "    else:\n",
        "        plt.imshow(data)\n",
        "    \n",
        "    plt.axis('off')\n",
        "    \n",
        "# Get the weights of the first convolutional layer \n",
        "# (so that's the second layer, just after the input layer)\n",
        "first_layer_weights = vgg_model.layers[1].get_weights()\n",
        "\n",
        "# first_layer_weights[0] stores the connection weights of the layer\n",
        "# first_layer_weights[1] stores the biases of the layer\n",
        "# For now we're just interested in the connections\n",
        "filters = first_layer_weights[0]\n",
        "\n",
        "# Visualise the filters \n",
        "# (swapaxes to get it to be in the appropriate ordering of dimensions)\n",
        "vis_square(np.swapaxes(filters, 0, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see how each filter detects a different property of the input image. Some are designed to respond to certain colours, while some other -- the greyscale looking ones -- detects changes in brightness such as edges. \n",
        "\n",
        "Another way of visualising the network is to see which neurons get activated as the images traverses the network. A neuron outputing a high value means the pattern it has learnt to detect has been observed. \n",
        "Let's apply this to our kitten image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# keras works with tensorflow but also with Theano, there are differences.\n",
        "import keras.backend as K\n",
        "\n",
        "def get_layer_output(model, image, layer):\n",
        "    output_fn = K.function([model.layers[0].input], [model.layers[layer].output])\n",
        "    return output_fn([image])[0]\n",
        "\n",
        "# retrieve the activation of the first convolutional layer\n",
        "layer_output = get_layer_output(vgg_model, img_t, 1)\n",
        "\n",
        "# visualise\n",
        "vis_square(np.swapaxes(layer_output, 0, 3), padsize=5, padval=1, activation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**It's worth spending a moment to understand what is going on here. Each pixel in this image is a different neuron in the neural network. Neurons on the same image sample share the same weights and therefore detect the same feature. You can compare the visualised filters above with their corresponding image sample. What filter helps detect grass?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Using this method, it is possible to visualise the deeper parts of the neural network, although they become much harder to interpret. You can visualise the output of the second convolutional layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the eighth layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we get further down the network, the representations become smaller in their spatial features thanks to the pooling layers. The final convolutional layers only have dimensions 14 by 14."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here to visualise the 28th layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training your own network\n",
        "\n",
        "Lets train a network! We're going to use the CIFAR10 dataset, in which the goal is to categorise images in one of 10 categories. ([more informations](https://www.cs.toronto.edu/~kriz/cifar.html))\n",
        "\n",
        "If you're interested in benchmarks, have a look [here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130). \n",
        "\n",
        "### Loading the CIFAR10 dataset\n",
        "\n",
        "Load the CIFAR10 dataset. Here you need similar steps than for MNIST:\n",
        "\n",
        "* use `cifar10.load_data()` to load the data (hopefully you have done that earlier)\n",
        "* separate train and test\n",
        "* normalise the values by 255\n",
        "* there are 10 categories so use `np_utils.to_categorical` to specify the output has 10 categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Add your code here to load and prepare the cifar10 data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the model\n",
        "\n",
        "Let's define a model, you will use a small model so that the training is not too slow. You should be able to recognise the key parts. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "cifar10_model = Sequential()\n",
        "\n",
        "cifar10_model.add(Conv2D(32, (3, 3), \n",
        "                         padding='same', \n",
        "                         input_shape=(32, 32, 3), activation='relu'))\n",
        "\n",
        "cifar10_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "cifar10_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cifar10_model.add(Dropout(0.25))\n",
        "\n",
        "cifar10_model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "cifar10_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "cifar10_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cifar10_model.add(Dropout(0.25))\n",
        "\n",
        "cifar10_model.add(Flatten())\n",
        "cifar10_model.add(Dense(512, activation='relu'))\n",
        "cifar10_model.add(Dropout(0.5))\n",
        "cifar10_model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Quiz: HOW MANY WEIGHTS IN THE NETWORK?**\n",
        "\n",
        "- How many convolution weights does the first layer contain? What about the second layer?\n",
        "- Are there any other weights in those layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the training schedule\n",
        "\n",
        "Using the Adam optimizer, you can compile the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the Adam optimizer, as before\n",
        "\n",
        "cifar10_model.compile(loss='categorical_crossentropy', \n",
        "                      optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image pre-processing\n",
        "\n",
        "We have to define the preprocessing for the images. Here we define:\n",
        "\n",
        "* 0-mean across all images (\"feature-centering\")\n",
        "* variance 1 across all images (\"normalization\")\n",
        "* we introduce a random horizontal and vertical shift to create more (\"perturbed\") training samples (makes the NN more robust as well)\n",
        "* randomly flip images horizontally\n",
        "\n",
        "you get the idea of what can be done. Don't forget that randomisation can improve things significantly but they also mean you have many (many) more samples to train on. It also means you're introducing (a lot) of correlation in your dataset whence the need for (more) regularisation such as dropout... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Preprocessing, does both normalization and augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=True,                 # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,                 # set each sample mean to 0\n",
        "        featurewise_std_normalization=True,      # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,      # divide each input by its std\n",
        "        rotation_range=0,                        # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,                   # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,                  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,                    # randomly flip images\n",
        "        vertical_flip=False)                     # randomly flip images\n",
        "\n",
        "# Compute quantities required for featurewise normalization (std, mean)\n",
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And you're set! You can start training and see the accuracy improve! (Don't feel like you have to wait all the way until the end, we did it for you and it gets to around 80% accuracy in a bit over an hour of training which is ok but quite far from state of the art result for this dataset). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "nb_epoch = 200\n",
        "\n",
        "cifar10_model.fit_generator(\n",
        "    datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
        "    steps_per_epoch=X_train.shape[0]/batch_size,\n",
        "    epochs=nb_epoch,\n",
        "    validation_data=(X_test, Y_test))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}