{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks for Sequences and Time Series\n",
        "\n",
        "Let's try to use the RNN for the Credit Card Fraud Detection problem. \n",
        "\n",
        "As before, this first cell sets up the notebook.\n",
        "Additionally, we load the file `utils/helpers.py` which defines \n",
        "\n",
        "* `train_test_split_time_series`\n",
        "* `reshape_to_batches` \n",
        "* `_3d_to_2d` \n",
        "\n",
        "that we defined in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "    \n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import keras\n",
        "\n",
        "# this is the same as copy pasting a bunch of \"def\" here.\n",
        "%run utils/helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the data\n",
        "\n",
        "Load the `creditcard.csv` data into a dataframe called `ccfd`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The utils functions define the same `train_test_split_time_series` you had used before. Use this function to build a train-test split and check the shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the usual scaling preprocessing (on both the training and test set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the `reshape_to_batches` function with batch size 100 and apply it to the training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reshape to batches\n",
        "BATCH_SIZE = 100\n",
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Remark\n",
        "Note that the batch size is particularly important because this is the sequence size that we are going to train the RNN on. \n",
        "This means that any dependencies further apart than`BATCH_SIZE` **will not be taken into account**. \n",
        "\n",
        "We could in theory give only one batch with the entire sequence but that will take an excessive amount of time to train and success is not guaranteed (vanishing gradient problem). \n",
        "\n",
        "### Re-encoding the data\n",
        "\n",
        "As in the previous data, create a `y_binary` with two columns (0, 1) and batch `yTrain`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create model\n",
        "\n",
        "In theory the RNN can read arbitrarily many time-steps, which is one of the reasons it can, theoretically, offer better performance than the CNN for time series. \n",
        "In practise however, it is limited by the vanishing gradient problem and the exploding computational requirement implied by taking increasingly many time-steps.\n",
        "\n",
        "The cell below imports key Keras layers:\n",
        "\n",
        "* `Input` and `Dense` which you already know\n",
        "* `SimpleRNN` and `TimeDistributed` which are helpful for time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# import all dependencies\n",
        "from keras.layers import Input, Dense, SimpleRNN, TimeDistributed\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the input layer with appropriate dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# add your code here for the input layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the architecture of the RNN\n",
        "\n",
        "By default, Keras considers the the **many-to-one** architecture, sometimes also known as an _encoder_. \n",
        "However, we want to perform a prediction at every time step. \n",
        "Therefore, we make the RNN layer return output for every sequence with `return_sequences=True`.\n",
        "\n",
        "The cell below, chained to the `inputs` layer, is an RNN cell.\n",
        "You should recognise a few things:\n",
        "\n",
        "* how many neurons are there? (or what's the dimensionality of the output of that layer?)\n",
        "* what's the activation function\n",
        "* the initializer is the Glorot intializer, centered at zero\n",
        "* no dropout\n",
        "\n",
        "the rest of the parameters don't really matter for now (we will modify some of them later) but feel free to have a look at the [documentation](https://keras.io/layers/recurrent/) for a definition of all the parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "rnn = SimpleRNN(64, \n",
        "                activation='tanh', \n",
        "                use_bias=True, \n",
        "                kernel_initializer='glorot_uniform',\n",
        "                recurrent_initializer='orthogonal', \n",
        "                bias_initializer='zeros', \n",
        "                kernel_regularizer=None,\n",
        "                recurrent_regularizer=None, \n",
        "                bias_regularizer=None, \n",
        "                activity_regularizer=None, \n",
        "                kernel_constraint=None, \n",
        "                recurrent_constraint=None, \n",
        "                bias_constraint=None, \n",
        "                dropout=0.0, \n",
        "                recurrent_dropout=0.0, \n",
        "                return_sequences=True, \n",
        "                return_state=False, \n",
        "                go_backwards=False, \n",
        "                stateful=False, \n",
        "                unroll=False)(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next cell is an output layer with 2 dimensions given that there are two classes (we're still in the classification context). \n",
        "\n",
        "Then, we wrap a model around the whole and compile it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "predictions = TimeDistributed(Dense(2, activation='softmax'))(rnn)\n",
        "\n",
        "rnn_model = Model(inputs=inputs, \n",
        "              outputs=predictions)\n",
        "\n",
        "rnn_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we're good to fit this for a few epochs and check the performances. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "rnn_model.fit(XTrain_s_batch, yTrain_batch, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation\n",
        "\n",
        "From the previous notebook, you know how to evaluate the performances of a model such as the one you just trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check the shapes of all relevant objects, reshape if necessary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# make the prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show the roc auc score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison with the CNN results\n",
        "\n",
        "Load the FPR and TPR from the CNN case, and show both the AUC of the RNN you've just trained as well as that of the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show the AUC for the CNN and the RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can observe, the RNN is globally better than the CNN here (the corresponding curve is everywhere above). \n",
        "AUC offer a nice way to compare between different classification models.\n",
        "\n",
        "**Note**: remain careful though, the AUC put emphasis on the *accuracy* but, as you know, in this case we may care more about fraud *recall*. \n",
        "Don't forget to also check the confusion matrices etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM\n",
        "\n",
        "One of the best parts of using Keras' functional API is that we can easily reuse components, let's replace the vanilla RNN with an LSTM.\n",
        "Again, you should recognise a few things, in fact pretty much everything is similar to the `SimpleRNN` you used before. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "# the implementation parameter determines whether your hardware is cpu (1) or gpu (2)\n",
        "lstm = LSTM(64, \n",
        "            activation='tanh', \n",
        "            recurrent_activation='hard_sigmoid', \n",
        "            use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', \n",
        "            recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', \n",
        "            unit_forget_bias=True, \n",
        "            kernel_regularizer=None, \n",
        "            recurrent_regularizer=None, \n",
        "            bias_regularizer=None, \n",
        "            activity_regularizer=None, \n",
        "            kernel_constraint=None, \n",
        "            recurrent_constraint=None, \n",
        "            bias_constraint=None, \n",
        "            dropout=0.0, \n",
        "            recurrent_dropout=0.0, \n",
        "            implementation=1,      # CPU or GPU\n",
        "            return_sequences=True, \n",
        "            return_state=False, \n",
        "            go_backwards=False, \n",
        "            stateful=False,\n",
        "            unroll=False)(inputs)\n",
        "\n",
        "#finally we give a 2 dimensional softmax output layer\n",
        "predictions = TimeDistributed(Dense(2, activation='softmax'))(lstm)\n",
        "\n",
        "lstm_model = Model(inputs=inputs, \n",
        "                   outputs=predictions)\n",
        "\n",
        "lstm_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# add your code to fit the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate the quality of the LSTM classifier\n",
        "\n",
        "Compare the LSTM to both the RNN and the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM vs GRU\n",
        "\n",
        "A last one we can test is the GRU. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "gru = GRU(64, \n",
        "          activation='tanh', \n",
        "          recurrent_activation='hard_sigmoid',\n",
        "          use_bias=True, \n",
        "          kernel_initializer='glorot_uniform',\n",
        "          recurrent_initializer='orthogonal', \n",
        "          bias_initializer='zeros',\n",
        "          kernel_regularizer=None, \n",
        "          recurrent_regularizer=None, \n",
        "          bias_regularizer=None,\n",
        "          activity_regularizer=None, \n",
        "          kernel_constraint=None, \n",
        "          recurrent_constraint=None,\n",
        "          bias_constraint=None, \n",
        "          dropout=0.0, \n",
        "          recurrent_dropout=0.0, \n",
        "          implementation=1,\n",
        "          return_sequences=True, \n",
        "          return_state=False, \n",
        "          go_backwards=False, \n",
        "          stateful=False, \n",
        "          unroll=False)(inputs)\n",
        "\n",
        "# output layer, as per usual\n",
        "predictions = TimeDistributed(Dense(2, activation='softmax'))(gru)\n",
        "\n",
        "# model compilation and fitting\n",
        "gru_model = Model(inputs=inputs, outputs=predictions)\n",
        "gru_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "gru_model.fit(XTrain_s_batch, yTrain_batch, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacking: combine NNs as lego blocks\n",
        "\n",
        "Just as with CNNs, RNN units can be stacked on top of each other to form a more involved model. \n",
        "Since the weights are shared in each RNN stack (layer), the hypothesis is that every stack forms both new features and a different time-scale at which it operates. \n",
        "\n",
        "Try to build two LSTM layers with the same settings as before, stack one after the other and test the whole lot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe that the training is now a bit slower, you have twice as many parameters after all... \n",
        "\n",
        "Check the performances as well as compared with the 1-layer LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So that's worse. \n",
        "Quite likely we have started to overfit...\n",
        "\n",
        "There are two ways to go about possible overfitting in the hope that a more complex model might lead to better performances (which is not necessarily true):\n",
        "1. decrease the number of parameters\n",
        "2. introduce regularisation\n",
        "\n",
        "Let's start by reducing the number of parameters `64-->32`, do exactly the same as before but with LSTMs with only 32 neurons per layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# add your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, that's better. \n",
        "\n",
        "You may get slightly different result but I currently have:\n",
        "\n",
        "* AUC LSTM 64    = 0.9734\n",
        "* AUC LSTM 64x64 = 0.9573 (-1.6 %)\n",
        "* AUC LSTM 32x32 = 0.9739 (+0.05 %) \n",
        "\n",
        "Of course, to be complete, you should also look at the fraud recall as we've already mentioned before.\n",
        "\n",
        "### Regularisaton\n",
        "\n",
        "With keras it is particularly easy to add any form of regularisation you want, either using the\n",
        "\n",
        "* `[component]_regularizer` parameter (penalise components that are too far from sensible values) or \n",
        "* the `[component]_constraint` parameter (clip components to be within a set range). \n",
        "\n",
        "In the first case, you can apply both `l1` and `l2` of the regularisation techniques you have learned so far [regulariser docs](https://keras.io/regularizers/).\n",
        "You can also add constraints (min norm, max norm, etc see [constrain docs](https://keras.io/constraints/))\n",
        "\n",
        "Of course, picking the parameters of the regularisation is hard and there is no good simple generic technique to do it. \n",
        "You could think about CV but here it would just be computationally too expensive. \n",
        "There are some rule of thumbs in terms of what is \"big\" and what is \"small\" but none are really justified. \n",
        "This is where resources can make all the difference.\n",
        "If you have access to a bunch of GPUs (or better, TPUs) training one neural net with a set of regularisation parameters can be done in a reasonable time and therefore you could do a form of randomised CV. \n",
        "If you're on a single CPU on your laptop however, you probably should not attempt doing hyperparameter tuning, your time is probably best invested buying credits off a cloud computing provider and using their GPUs paying per hour of use. \n",
        "\n",
        "Let's try to see if some basic regularisation will help in the double-LSTM case with 64. \n",
        "\n",
        "* add a l1_l2 regulariser (`keras.regularizers.l1_l2`) with parameter `0.01` for the `kernel_regularizer`\n",
        "* do the same for the `recurrent_regularizer`\n",
        "* keep 64 neurons on both layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's a very significant performance drop... again you can get a sense for the difficulty of tuning a multi-layered Neural Net...\n",
        "\n",
        "Before we quit on regularisation altogether, we have to consider the reasons behind such a significant drop. \n",
        "It could be either that we applied an unreasonably high regularisation value or that it is much harder to optimise the problem with regularisation and the optimisation algorithm needs more epochs... \n",
        "\n",
        "You could try the latter by going from 15 to 25 or 50 epochs and you will see that you will obtain performances comparable with the other algorithms (though it will take much more time). \n",
        "\n",
        "### Dropout\n",
        "\n",
        "One of the most effective forms of regularisations in the context of Neural Networks is Dropout.\n",
        "There are two places where we can use dropout:\n",
        "\n",
        "- on the input connection\n",
        "- on the reccurent connections.\n",
        "\n",
        "a dropout on the connection means that the data on that connection to each LSTM cell will be excluded from node activation and weight updates with a given probability. \n",
        "The dropout value is a percentage between 0 (no dropout) and 1 (no connection).\n",
        "\n",
        "Get back to your 2-layer 64 LSTM and some regularisation and add\n",
        "\n",
        "* a dropout with parameter 0.2\n",
        "* a recurrent_dropout with parameter 0.05\n",
        "\n",
        "you will need at least 25 epochs to get decent results. \n",
        "\n",
        "(why these parameters? well...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "well well... \n",
        "\n",
        "The take home message here is a bit disappointing but very important: regularisation is difficult to tune, requires a lot of practice and it doesn't hurt to have large computational resources...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bi-directional RNN\n",
        "\n",
        "A RNN can be run simultaneously from \"both directions\":\n",
        "\n",
        "* one \"forward\" in time\n",
        "* one \"backward in time\n",
        "\n",
        "if you think about a sentence as being a sequence of words then that would amount to having one RNN reading the words as you would naturally and the other one reading the sentence backward (which can be useful in languages that reject verbs to the end of the sentence for example). \n",
        "\n",
        "In the context of credit card fraud detection, it is not very appropriate (we want to learn online, as new transactions come in, and not a posteriori) but we can still show how it works. \n",
        "\n",
        "After our rather unsuccessful attempt with regularisation we'll keep things simple and just duplicate the LSTM cell that had worked well before, one forward, one backward. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Forward cell\n",
        "lstm_fwd = LSTM(64, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
        "            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
        "            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n",
        "            ### GO FORWARD\n",
        "            go_backwards=False, stateful=False, unroll=False)(inputs)\n",
        "\n",
        "# Note two important things\n",
        "# 1) we turn on the go_backwards parameter\n",
        "# 2) we give the same input (inputs) to the backward LSTM\n",
        "lstm_bck = LSTM(64, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
        "            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
        "            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n",
        "            ### GO BACKWARD\n",
        "            go_backwards=True, stateful=False, unroll=False)(inputs)\n",
        "\n",
        "# now we have to combine the results of the two layers\n",
        "# we can use different options, but the most common one\n",
        "# is to concatenate them together\n",
        "merge = keras.layers.Concatenate(axis=-1)([lstm_fwd, lstm_bck])\n",
        "\n",
        "predictions = TimeDistributed(Dense(2, activation='softmax'))(merge)\n",
        "bidir_model = Model(inputs=inputs, \n",
        "                    outputs=predictions)\n",
        "bidir_model.compile(optimizer='rmsprop',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "bidir_model.fit(XTrain_s_batch, yTrain_batch, epochs=15)\n",
        "y_pred_bidir = bidir_model.predict(XTest_s_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add your code to compare models\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}