{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RNN for Regression\n",
        "\n",
        "Now that you have seen how to use of RNN for classification, we will consider the problem of time-series regression.\n",
        "We will look at real data corresponding to the price of a cryptocurrency and will try to predict it.\n",
        "\n",
        "**Note**: we will be training a lot of networks and the output might clog your jupyter notebook. Remember that you can click in the left margin of an output in order to \"hide\" it.\n",
        "\n",
        "The next cell does the usual setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error,mean_squared_error\n",
        "    \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Input, Dense, LSTM, SimpleRNN, TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "# this is the same as copy pasting a bunch of \"def\" here.\n",
        "%run utils/helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the data\n",
        "\n",
        "Load the data `./data/ethereum_dataset` and have a look. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth = pd.read_csv('./data/ethereum_dataset.csv',)\n",
        "print(eth.shape)\n",
        "eth.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### NaNs\n",
        "\n",
        "How many features contain NaNs and where do the NaNs come from?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth.apply(pd.isnull).sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the `eth_ens_register` has quite a few NaNs as compared to the number of rows in the dataset (~600 vs ~800). \n",
        "Should you just drop this feature?\n",
        "\n",
        "To investigate this further, we will first set the `Date(UTC)` as index then investigate this feature in more depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Date/Time\n",
        "\n",
        "Take the `Date(UTC)` feature, make it into a datetime object thanks to `pd.to_datetime` and use it as an index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "eth[\"Date(UTC)\"] = pd.to_datetime(eth[\"Date(UTC)\"])\n",
        "eth = eth.set_index(\"Date(UTC)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Back to NaNs\n",
        "\n",
        "We saw that `eth_ens_register` (the domain name service for an object on the enthereum blockchain) is `NaN`.\n",
        "As usual we have to deal with missing values and see if we should just drop that feature or maybe impute a sensible value. \n",
        "\n",
        "Start by checking the values for `eth_ens_register`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "eth[\"eth_ens_register\"].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It turns out there are entries, which are exactly 0, have a look at the corresponding slice of the data, can you spot anything?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth[eth[\"eth_ens_register\"]==0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All of these records are on or after the 3rd of May 2017. \n",
        "Most probably the ENS did not exist before that, let's confirm by checking the dates corresponding to the `NaN` values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth[eth[\"eth_ens_register\"].isnull()].tail(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks like our hypothesis is confirmed. \n",
        "Since we will try to predict future values of ethereum, when the ENS exist, we can safely set them to 0. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth.loc[eth[\"eth_ens_register\"].isnull(), \"eth_ens_register\"] = 0\n",
        "\n",
        "# you can check it worked as expected\n",
        "eth[eth[\"eth_ens_register\"]==0].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis\n",
        "\n",
        "Have a look at some of the features, in particular have a look at the following features and discuss\n",
        "\n",
        "* `eth_etherprice`\n",
        "* `eth_supply`\n",
        "* `eth_tx`\n",
        "* `eth_difficulty`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "eth[\"eth_etherprice\"].plot()\n",
        "plt.title(\"ETHERPRICE\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "eth[\"eth_supply\"].plot()\n",
        "plt.title(\"SUPPLY\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "eth[\"eth_tx\"].plot()\n",
        "plt.title(\"TX\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "eth[\"eth_difficulty\"].plot()\n",
        "plt.title(\"DIFFICULTY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the \"Bloom Baster\", on October 16, 2017, Ethereum was \"renewed\" which means that the reward for mining a block fell from 5 `eth` to 3 but the block time was also reduced. \n",
        "Because of this, `difficulty` dropped. \n",
        "If you're not into this cryptocurrency mumbo jumbo, note the discontinuity in \"difficulty\" and think about what that means for any predictive method using this data.\n",
        "\n",
        "Show that period specifically a drop in `difficulty`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth['2017-10':'2017-11'][\"eth_difficulty\"].plot(marker=\"o\")\n",
        "plt.ylabel(\"Difficulty\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can clearly see the drop.\n",
        "\n",
        "Apply a standard scaling to the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "])\n",
        "\n",
        "X = pipeline.fit_transform(eth.values)\n",
        "print(X.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the model\n",
        "\n",
        "We want to be able to predict the price tomorrow (prediction horizon = 1 day) given everything but the price time series. \n",
        "Let's start by preprocessing all the values: scale the data. \n",
        "\n",
        "Now the $y$-s become tomorrow's price values, but we also have to drop our last data point.\n",
        "\n",
        "We'll give you that one, make sure it makes sense (especially the indices). \n",
        "Note that we'll exclude the price feature from now on to make it \"more interesting\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "id_price = 1 # index of the appropriate column\n",
        "y = np.expand_dims(X[1:, id_price], -1)\n",
        "\n",
        "#initially we are going to exclude the price feature from the data set entirely\n",
        "X_ = X[0:-1, np.arange(X.shape[1]) != id_price]\n",
        "\n",
        "print(X_.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One of the main limitations of neural networks is they typically need a ton of data for training. \n",
        "831 transaction is by no means enough. \n",
        "\n",
        "A (sub-optimal) way around this is to decrease the amount of testing data. \n",
        "An obvious caveat is that we could be testing on a very particular test set, not seen in the training data, which could lead to a disappointingly poor generalisation error.\n",
        "\n",
        "The cell below defines a training-testing split for arrays corresponding to time series, it should all look familiar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def train_test_split_time_series_regres(X, y, test_ratio=0.15):\n",
        "    total_samples = X.shape[0]\n",
        "    train_idx = int(total_samples * (1-test_ratio))\n",
        "    XTrain = X[:train_idx]\n",
        "    yTrain = y[:train_idx]\n",
        "    XTest = X[train_idx:]\n",
        "    yTest = y[train_idx:]\n",
        "    return XTrain, yTrain, XTest, yTest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "XTrain, yTrain, XTest, yTest = train_test_split_time_series_regres(X_, y)\n",
        "print(XTrain.shape)\n",
        "print(yTrain.shape)\n",
        "print(XTest.shape)\n",
        "print(yTest.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 30\n",
        "XTrain_batch = reshape_to_batches(XTrain, BATCH_SIZE)\n",
        "yTrain_batch = reshape_to_batches(yTrain, BATCH_SIZE)\n",
        "print(XTrain_batch.shape)\n",
        "print(yTrain_batch.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 30\n",
        "XTest_batch = reshape_to_batches(XTest, BATCH_SIZE)\n",
        "yTest_batch = reshape_to_batches(yTest, BATCH_SIZE)\n",
        "print(XTest_batch.shape)\n",
        "print(yTest_batch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#import all dependencies\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we are going to make one little change. \n",
        "Because we would to be able to predict the price of ethereum given only yesterday's information, we will allow our network to accept batches of **any size**. \n",
        "This is accomplished by supplying `None` as a shape parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(None, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now add an LSTM layer with 64 neurons and no regularisation.\n",
        "\n",
        "Note that\n",
        "\n",
        "* the output layer must have a `linear` activation function (it's a regression, not a classification that we're doing)\n",
        "* the loss is not the cross entropy loss but the `mean_squared_error` loss\n",
        "\n",
        "fit the model with 100 epochs (it will be very quick)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "lstm = LSTM(64, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
        "            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
        "            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n",
        "            go_backwards=False, stateful=False, unroll=False)(inputs)\n",
        "\n",
        "# the main difference with classification is that the activation of the last layer is linear\n",
        "predictions = TimeDistributed(Dense(1, activation='linear'))(lstm)\n",
        "\n",
        "lstm_model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# the second difference is the loss function (now MSE)\n",
        "lstm_model.compile(optimizer='rmsprop',\n",
        "                   loss='mean_squared_error',\n",
        "                   metrics=['accuracy'])\n",
        "lstm_model.fit(XTrain_batch, yTrain_batch, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate predictions and check the mean squared error!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_lstm = lstm_model.predict(XTest_batch)\n",
        "\n",
        "print(yTest.shape)\n",
        "print(y_pred_lstm.shape)\n",
        "\n",
        "print(\"MAE: {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n",
        "print(\"MSE: {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These values don't really tell you much at this point. \n",
        "Remember that you have normalised the values with 0-mean and variance 1 so that the small numbers should not be particularly impressive.\n",
        "\n",
        "We can get a better feeling of the quality of our predictions via some visualisations.\n",
        "\n",
        "Plot the predicted values vs the true values and discuss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(yTest, _3d_to_2d(y_pred_lstm)[:125])\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
        "plt.xlabel('Measured', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Right... so that's not that great, it looks like our model is generally predicting higher values (bias).\n",
        "On a positive note, it looks like it is capturing the trend. \n",
        "\n",
        "### Adjusting the architecture\n",
        "\n",
        "Since training is very fast here, you can play around with the number of neurons and number of layers to try to get better results. \n",
        "One way to go is to increase the number of neurons (say to 256); feel free to tweak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(None, 16))\n",
        "lstm = LSTM(256, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
        "            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
        "            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n",
        "            go_backwards=False, stateful=False, unroll=False)(inputs)\n",
        "\n",
        "predictions = TimeDistributed(Dense(1, activation='linear'))(lstm)\n",
        "lstm_model256 = Model(inputs=inputs, outputs=predictions)\n",
        "lstm_model256.compile(optimizer='rmsprop',\n",
        "                      loss='mean_squared_error',\n",
        "                      metrics=['accuracy'])\n",
        "lstm_model256.fit(XTrain_batch, yTrain_batch, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating your model\n",
        "\n",
        "Copy paste the cell to evaluate your model adjusting for the new model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_pred_lstm256 = lstm_model256.predict(XTest_batch)\n",
        "\n",
        "print(\"MAE LSTM 64:  {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n",
        "print(\"MSE LSTM 64:  {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n",
        "print(\"MAE LSTM 256: {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm256)[:125])))\n",
        "print(\"MSE LSTM 256: {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm256)[:125])))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(yTest, _3d_to_2d(y_pred_lstm)[:125], color=\"r\", \n",
        "         ls=\"none\", marker=\"o\", markersize=4, alpha=0.2, label=\"lstm64\")\n",
        "plt.scatter(yTest, _3d_to_2d(y_pred_lstm256)[:125], label=\"lstm256\")\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
        "plt.xlabel('Measured', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.legend(fontsize=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok so that's better but not hugely so.\n",
        "\n",
        "Try something more complicated, for example:\n",
        "\n",
        "* two layers\n",
        "* some dropout\n",
        "* some regularisation\n",
        "* more epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(None, 16))\n",
        "\n",
        "lstm = LSTM(64, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, \n",
        "            # -- reg \n",
        "            kernel_regularizer=regularizers.l2(0.001), \n",
        "            recurrent_regularizer=regularizers.l2(0.001), \n",
        "            bias_regularizer=None, activity_regularizer=None, \n",
        "            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, \n",
        "            # -- dropout\n",
        "            dropout=0.3, \n",
        "            recurrent_dropout=0.04, \n",
        "            implementation=1, return_sequences=True, return_state=False, \n",
        "            go_backwards=False, stateful=False, unroll=False)(inputs)\n",
        "\n",
        "lstm1 = LSTM(64, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, \n",
        "            # -- reg\n",
        "            kernel_regularizer=regularizers.l2(0.001), \n",
        "            recurrent_regularizer=regularizers.l2(0.001), \n",
        "            bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
        "            recurrent_constraint=None, bias_constraint=None, \n",
        "            # -- dropout\n",
        "            dropout=0.3, \n",
        "            recurrent_dropout=0.04, \n",
        "            implementation=1, return_sequences=True, return_state=False, \n",
        "            go_backwards=False, stateful=False, unroll=False)(lstm)\n",
        "\n",
        "dense =  TimeDistributed(Dense(64, activation='sigmoid'))(lstm1)\n",
        "predictions = TimeDistributed(Dense(1, activation='linear'))(dense)\n",
        "model_lstm256x256r = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model_lstm256x256r.compile(optimizer='rmsprop',\n",
        "                           loss='mean_squared_error',\n",
        "                           metrics=['accuracy'])\n",
        "model_lstm256x256r.fit(XTrain_batch, yTrain_batch, epochs=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_pred_lstm256x256r = model_lstm256x256r.predict(XTest_batch)\n",
        "\n",
        "print(\"MAE LSTM 64:       {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n",
        "print(\"MSE LSTM 64:       {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm)[:125])))\n",
        "print(\"MAE LSTM 256:      {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm256)[:125])))\n",
        "print(\"MSE LSTM 256:      {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm256)[:125])))\n",
        "print(\"MAE LSTM 256x256r: {0:.2f}\".format(mean_absolute_error(yTest, _3d_to_2d(y_pred_lstm256x256r)[:125])))\n",
        "print(\"MSE LSTM 256x256r: {0:.2f}\".format(mean_squared_error(yTest, _3d_to_2d(y_pred_lstm256x256r)[:125])))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(yTest, _3d_to_2d(y_pred_lstm)[:125], color=\"r\", \n",
        "         ls=\"none\", marker=\"o\", markersize=4, alpha=0.2, label=\"lstm64\")\n",
        "plt.plot(yTest, _3d_to_2d(y_pred_lstm256)[:125], color=\"c\", \n",
        "         ls=\"none\", marker=\"o\", markersize=4, alpha=0.4, label=\"lstm256\")\n",
        "plt.scatter(yTest, _3d_to_2d(y_pred_lstm256x256r)[:125], label=\"lstm256x256r\")\n",
        "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
        "plt.xlabel('Measured', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.legend(fontsize=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try some other methods to validate our results. \n",
        "\n",
        "Look at the actual values and compare with the target ones. Remember that we scaled all of the features. To rescale just the price, we are going to pad the rest of the values with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(_3d_to_2d(y_pred_lstm256x256r)[:125])\n",
        "plt.plot(yTest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The network has learned to capture the general trend, and it has learned that at a certain condition there is a spike but it overestimates the spikes. \n",
        "\n",
        "## Generator\n",
        "\n",
        "Now that we have an (more or less) acceptable machinery we can apply the same approach to not only predict the price but also all other features (i.e. multi-dimensional output). \n",
        "\n",
        "To do that instead of a shifted price, we are going to use as target values all features shifted by one (one day horizon). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_shift = X[1:, :] # the \"future\"\n",
        "X_shift = X[0:-1, :] # the \"past\"\n",
        "print(X_shift.shape)\n",
        "print(y_shift.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to put aside the testing here (bad!) and focus on trying to have a good model for the problem above and interpret the results. \n",
        "By using all the data we should have the best fit though we will not be able to give any guarantees about generalisability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# reshape to expected output\n",
        "BATCH_SIZE = 30\n",
        "X_batch = reshape_to_batches(X_shift, BATCH_SIZE)\n",
        "y_batch = reshape_to_batches(y_shift, BATCH_SIZE)\n",
        "print(X_batch.shape)\n",
        "print(y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the net\n",
        "\n",
        "Let's use as a first layer a 512 LSTM without regularisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(None, 17)) # This returns a tensor\n",
        "lstm = LSTM(512, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
        "            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
        "            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
        "            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
        "            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, \n",
        "            recurrent_dropout=0.0, implementation=1, return_sequences=True, return_state=False, \n",
        "            go_backwards=False, stateful=False, unroll=False)(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only thing which we need to change is the output dimensions of the network from 1 to 17. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "predictions = TimeDistributed(Dense(17, activation='linear'))(lstm)\n",
        "model_lstm512 = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model_lstm512.compile(optimizer='rmsprop',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's turn the magic on, train for 500 epochs, more may be better but may crash your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model_lstm512.fit(X_batch, y_batch, epochs=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying to predict the future\n",
        "\n",
        "In order to make our network tell the future, we need to make it generate the values for tomorrow and then iteratively feed this back into the network.\n",
        "The last data point is from November 7, let's try to predict until the end of January (very dubious from a statistical perspective but we can try). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#remember that we left out the last X sample, we can start from there\n",
        "days = 3 * 30\n",
        "\n",
        "X_last = X[-1, :]\n",
        "X_batch = np.swapaxes(np.expand_dims(np.expand_dims(X_last, -1), -1), 0, 2)\n",
        "\n",
        "for day in range(days):\n",
        "    print(\"Day #{} - {} data points\".format(day, X_batch.shape[1]))\n",
        "    y_pred = model_lstm512.predict(X_batch)\n",
        "    # we are only going to use the most recent prediction\n",
        "    # otherwise the prediction power could quickly deteriorate\n",
        "    y_pred = np.swapaxes(np.expand_dims(np.expand_dims(y_pred[0, X_batch.shape[1]-1, :], -1), -1), 0, 2)\n",
        "    X_batch = np.concatenate([X_batch, y_pred], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "        pipeline.inverse_transform(\n",
        "            X_batch.reshape(X_batch.shape[1], X_batch.shape[2])))\n",
        "df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0], unit='s')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Very interesting, it seems like our model has not even learned that the day increments at every time step. \n",
        "Do you thing this is a bug or a feature? \n",
        "\n",
        "Let's try to give it the entire history to check. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#remember that we left out the last X sample, we can start from there\n",
        "days = 3*30\n",
        "\n",
        "X_last = X[:, :]\n",
        "X_batch = np.swapaxes(np.swapaxes(np.expand_dims(X_last, -1), 0, 2), 1, 2)\n",
        "print(X_batch.shape)\n",
        "\n",
        "for i in range(days):\n",
        "    print(\"Day #{} - {} data points\".format(i,X_batch.shape[1]))\n",
        "    y_pred = model_lstm512.predict(X_batch)\n",
        "    y_pred = np.swapaxes(np.expand_dims(np.expand_dims(y_pred[0,X_batch.shape[1]-1,:], -1), -1), 0, 2)\n",
        "    X_batch = np.concatenate([X_batch,y_pred], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(\n",
        "        pipeline.inverse_transform(\n",
        "                X_batch.reshape(X_batch.shape[1], X_batch.shape[2])))\n",
        "df.iloc[:,0] = pd.to_datetime(df.iloc[:, 0], unit='s')\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yes, it is confirmed, it is trying to learn the days as a function of the other feature. \n",
        "This is an illustration that the NN is trying to learn the **joint probability** of the features. \n",
        "\n",
        "In this case, the network seems to have learned that a price of between `$`285-300, along with other features are usually seen around October, so it predicts October as the date. \n",
        "Notice that it has learned that the time should also increase, and in fact the October 04th dates increase with around 30-40 minutes\n",
        "\n",
        "Let's have a look of how our model envisions the price change compared to the general trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.iloc[:,1].plot()\n",
        "df.iloc[-90:,1].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#plot just the prediction time series\n",
        "df.iloc[-90:,1].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Right...\n",
        "\n",
        "So now you see that it doesn't matter how fancy your model is, time series prediction is still very hard.\n",
        "In fact recently a (tongue-in-cheek?) [paper](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889) shows that old-fashioned stats models can do much better than fancy ML algorithms for forecasting. \n",
        "It still needs to be confirmed and tested but it is an interesting result to keep in mind: predicting the future is hard, no-one really knows for sure how to do it, and if someone does, they're rich and unlikely to share the information. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}